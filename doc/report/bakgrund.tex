\section{Bakgrund}
Under \emph{BlueHat} 2019 gav \emph{MSRC} (\emph{Microsoft Security Response Center}) en överblick
över Microsofts taktiker för hur de hanterar säkerhetsbrister i deras produkter och tjänster.
\cite{miller19} Taktikerna som listades var att
\begin{itemize}
	\item eliminera säkerhetsbristerna från början;
	\item implementera metoder för att försvåra utnyttjande av säkerhetsbrister;
	\item minimera platserna där attackerarna kan göra skada och förhindra åtkomst; samt slutligen
	\item minimera tidsfönstret då attackerare har tillgång till systemet med hjälp av aktiv övervakning.
\end{itemize}
Enligt Miller \cite{miller19} härstammar ungefär 70 procent av alla Microsofts CVEr från minnessäkerhetsbuggar. Om
hela klasser av säkerhetsbrister kan eliminieras genom att utveckla nya verktyg som underlättar för
utvecklare att finna dem kan det leda till stor påverkan på antalet sårbarheter.

% probably need a source for this paragraph
Det är särskilt fördelaktigt att undersöka minnessäkerhetsbuggar genom att
betrakta maskinkod. Genom att betrakta maskinkod bildar man en lågnivåförståelse
av en binär eftersom den fullständiga målkoden som kompilatorn genererat
existerar på denna nivån. Med en lågnivåförståelse kan man också härleda hur
binären interagerar med minnet genom att bland annat analysera minneslayouten
och den underliggande datastrukturen för att hitta problem vid minnesallokering
och minnesdeallokering, och därigenom hitta minnessäkerhetsbuggar.

Det finns ett flertal metoder för att analysera en exekverbar binär. Exempel på dessa är att: 
\begin{enumerate}
  \item disassemblera binären och läsa dess funktioner för att förstå vad de gör.
  \item dekompilera assemblykoden med ett verktyg som ger pseudokod, och sedan läsa denna mer
    läsbara koden.
  \item köra binären på speciella testfall och jämföra svaret med vad som förväntas. Om
    programmet implementerar en specifikation kan en existerande testsamling användas.
  \item fuzztesta binären, det vill säga automatiskt generera testfall tills ett orsakar en crash eller
    annat oönskat beteende i binären. Många fuzztestmotorer skapar testfall med en evolutionär
    algoritm, och många använder instrumentering över vilka programhopp som tas för att bedöma
    testfalls nyttighet.
  \item använda concolic testing, alltså fuzzing där en SMT solver genererar nya testfall genom att
    lösa för testfall som orsakar annorlunda programhopp.
  \item stega igenom programmet i en debugger för att se exakt vad programmet gör med viss input.
\end{enumerate}

Problematisk minneshantering har potential att påverka ett programs korrekthet och 
kan utnyttjas av fientliga aktörer i skadliga syften. Att minne hanteras på ett 
osäkert sätt är inte ovanligt, speciellt då proggrammet är skrivet i ett språk som är 
"memory unsafe" som exempelvis C/C++. Det är då lätt att vid utveckling av program 
göra misstag som introducerar sårbarheter, och kan vara svårare att upptäcka dessa 
sårbarheter när de väl introducerats, speciellt om det inkorrekta beteendet endast 
uppstår under körning med specifika indata.

Att läsa källkod är ett sätt att förstå program, men ibland är det gynnsamt att istället betrakta
maskinkoden direkt. Detta kan vara för att

Begreppet \textit{reverse engingeering} syftar på processen att söka insikt i hur en produkt 
(enhet/process/mjukvara/verktyg/system) arbetar, utan en etablerad insikt i dess interna 
uppbyggnad. Med andra ord syftar reverse engineering på att dekonstruera en produkt för att 
öka förståelsen av den. Detta görs genom att med olika metoder plocka isär produkten för 
att förstå hur den utför ett arbete. Reverse engineering är ett fundamentalt verktyg då insikt 
om en produkts design behövs men designspecifikationer ej existerar eller är tillgängliga. 
Reverse engineering har flera användningsområden, däribland då äldre produkter, vars design 
inte längre är tillgänglig, behöver undersökas, eller när funktionalitet försvunnit i 
utvecklingsaproccesen och behöver återfinnas. Reverse engineering är också användbart för 
att analysera fel som uppstår, för att förbättra delkomponenter eller för att diagnostisera 
en produkt.

För att bilda en allmän förståelse om ett program krävs både \textit{korrekt} och
\textit{abstrakt} förståelse. I detta avseende syftar \textit{korrekt} på
avsaknaden av felaktiga slutsatser och \textit{abstrakt} på möjligheten att
resonera om programmet generellt i motsats till att resonera om en specifik
konkret indata i taget.

% Metod 1-2, att läsa kod, kan ge en \textit{abstrakt} förståelse av vad
% programmet gör, men för att verifiera att huruvida resonemanget är korrekt krävs
% hypotestestning vilket kräver att programmet körs. Således går det inte att
% bilda en \textit{korrekt} förståelse genom att enbart läsa kod.

% Metod 3-5, att köra programmet på testfall, ger framförallt en
% black-box-förståelse av programmet. Tillgången till binären och
% exekveringsmiljön används endast som ett verktyg för att generera nya testfall.
% Fuzzing och concolic testing kan köras helautomatiskt och är \textit{korrekta}.
% Men ofta är en tillräckligt täckande sökning av indatarummet omöjlig, och då kan
% den automatiska analysen ha missat ett kvalitativt annorlunda beteende. Dessutom
% ger en omfattande uppsättning indata-utdata-par inte användaren samma
% information som källkoden ger. Därmed är helautomatiska analysmetoder inte
% \textit{abstrakta}. Notera att det inte nödvändigtvis tyder på en brist i den
% automatiska analysen att ett kvalitativt annorlunda beteende missas, för det
% gömda beteendet skulle kunna vara en konsekvens av komplicerad kod, som till
% exempel ett hoppvilkor beroende på en kryptografisk hash av indatan. Men en
% analysmetod borde kunna peka ut var dess förståelse tar slut, snarare än att
% utelämna detta fullständigt vilket är vad avsaknaden av testfall visar sig som.

% Med metod 6, en debugger, kan användaren följa exekveringen för en viss indata
% utan att riskera att missförstå hur datan transformeras. Om användaren har ett
% oändligt tålamod kan de göra detta om och om igen för olika indata genererade
% med till exempel fuzzing. Varje genomstegning ger information om koden som
% behandlar indatan men också viss information om övrig kod -- till exempel kan
% ett svårtaget hopp indikera en plats för användaren att rikta sin uppmärksamhet
% mot. Detta ger en både \textit{korrekt} och \textit{abstrakt} förståelse, men
% med en orimlig manuell arbetsbörda för användaren.

En helautomatisk \textit{korrekt} metod kan ge en \textit{abstrakt} förståelse
om processens förlopp visualiseras för användaren. Valet mellan manuell
arbetsbörda som ger djup förståelse och en testfallsgenerationsdriven process
som ger översiktlig förståelse kan genomföras av användaren om verktygen stödjer
hela spektrummet.

För att klargöra distinktionen mellan manuell och automatiska metoder för
binäranalys används följande use cases:

\begin{lstlisting}[
    label={list:first},
    language=Python,
    caption=Use case manuella metoder,
    frame=single
    ]
# Givet sträng-input från stdin
s = input()
if sha256(s) = "välkänd hash":
  print("Winner winner chicken dinner")
else:
  print("you lose")

\end{lstlisting}

I use cases där det existerar kända konstanter, något som är typiskt i fall som
involverar kryptografi i olika utsträckning, är det rimligt att tillämpa
manuella metoder för att bilda förståelse om programmet. Genom att inspektera
ett binärt program innehållande ovan källkod kan det enkelt hittas en konstant
associerad till sha-256 algoritmen och därmed bilda förståelse om programmet.

I ovan fall är det dessutom orimligt att tillämpa automatiska metoder eftersom
dessa, såsom concolic testing, genererar alltför stora symboliska
representationer och hade i det ovan exemplet krävt att det går att hitta
inversen till en given SHA-256 hash vilket idag är omöjligt och leder därmed
till att alla paths inte undersöks.

\begin{lstlisting}[
    label={list:first},
    language=Python,
    caption=Use case manuella metoder,
    frame=single
    ]
# Givet sträng-input från stdin
s = input()
if s == "secret":
  print("Winner winner chicken dinner")
else:
  print("you lost")
\end{lstlisting}

Ett motsatt fall är ovan och lämpas väl att undersökas med automatiska
metoder eftersom det är tidskrävande att manuellt välja slumpvalda värden på s
för att hitta den korrekta branchen. Istället lämpar concolic testing, dvs en
automatisk metod, sig väl i detta fallet eftersom concolic testing väljer olika
konkreta värden samtidigt som den tillämpar symbolisk exekvering med symboliska
värden som följer den givna branchen, t.ex. om \lstinline{s == "hej"} vilket
motsvarar att programmet väljer else-branchen och printar \lstinline{"you lost"}. 
Detta upprepas med nya exekveringsvägar och till slut hittas vilken input som ger
\lstinline{print("you won")}-branchen. 

\subsubsection{Symbolisk Exekvering}
Att exekvera ett program symboliskt innebär att representera värden utefter 
programflödet som symboliska restriktioner, vilka kan lösas av automatiserade 
teoremlösare (\emph{SMT solver}). En symbolisk körning representerar flera konkreta 
körningar eftersom de (symboliska) värden som används representerar grupper av 
konkreta värden vilka har gemensamt hur de påverkar programmets flöde. 

Vägar i programmets kontrollflöde utforskas med symboliska uttryck för de 
begränsningar som finns på programmets variablar - vilka egenskaper de måste uppnå 
för att just denna väg ska kunna följas. Eftersom de symboliska värdena har kapacitet 
att representera grupperingar av konkreta värden istället för enskilda sådana, 
utförs en generaliserad testning av programmet, som ger insikt i hur programmet 
beteer sig givet en grupp av parametrar som alla på grund av någon eller några 
gemensamma egenskaper, orsakar gemensamma beteenden i programmet. 

En symbolisk exekveringsmotor arbetar genom att först representera programmets input 
som symboliska variablar, vilka vid starten inte har några begränsningar, och när 
programflödet når en förgrening som baseras på någon av de symboliska variablerna, 
så väljer motorn en gren och tillsätter den grenens restriktioner på den symboliska 
variabeln för alla vägar som fortsätter utefter förgreningen. Operationer på värden 
under körningens väg översätts till symboliska operationer på motsvarande symboliska 
variabler. \cite{klee} När körning utefter grenen är slutförd så kan motorn börja om 
vid förgreningen och utforska andra alternativ med samma metodik. De tillståndsvillkor 
som en viss väg visas ha byggs därför successivt upp genom att motorn utökar de 
symboliska variablerna till villkorliga uttryck allt eftersom vägen följs. 

De fel som hittas genom symbolisk exekvering är tillförlitliga då metoden ej ger falskt 
positiva resultat. Huruvida villkorsblock av program är nårbara kan evalueras med 
säkerhet eftersom de krav som måste uppfyllas för att följa vägen dit dokumenteras under 
den symboliska körningen, och resulterar i fullständiga symboliska representationer 
som en automatiserad teoremlösare (SMT solver) kan appliceras på. 

Symbolisk exekvering är användbart för att resonera kring hur programmet beter sig 
beroende på grupperingar av input. För program där få värden tar gemensamma vägar blir 
fördelen över att istället använda dynamisk testning av konkret data dock låg. 

Eftersom symbolisk exekvering kan leda till \emph{path explosion} är det inte effektivt 
att alltid undersöka alla förgreningar i ett program. Exempel på metoder för att undvika 
path explosion är \emph{state-merging} och \emph{heuristics}. 

\subsubsection{Statisk och dynamisk binäranalys}
En annan typ av kategorisering av olika analysmetoder som fokuserar på hur
analysen genomförs delar metoderna i två grupper: statisk och dynamisk analys
\cite{dynamic_bin_analysis}.

Statisk analys syftar på analys som går att göra utan att exekvera programmet
som analyseras. Exempel på statisk binäranalys är metod 1-2, alltså att
disassembla binären och/eller visualisera kod \cite{dynamic_bin_analysis}.

Dynamisk analys går ut på att analysera ett program under exekvering
\cite{dynamic_bin_analysis}. Exempel på dynamisk binäranalys är metod 3-6. I
alla fall krävs någon typ av injektion av kod eller data i programmet i syfte
att kunna extrahera viktig information under programmets körning
\cite{dynamic_bin_analysis}. Många avancerade dynamiska metoder som t.ex.
concolic testing kräver symbolisk exekvering som går ut på att tilldela
symboliska värden till variabler och se hur dessa påverkar programhopp och
förgreningar och vad för möjliga värden som denna symbol kan inneha under
exekvering. I fallet med concolic testing används denna information för att,
med hjälp av en SMT-solver ta fram konkreta värden som leder till att
programmet kör till en program-distination som består av ett krasch.

\subsubsection{Existerande verktyg}
Det kan finnas flera metoder för analys av binärprogram och det finns ganska
många verktyg idag som stödjer ett eller flera av dessa metoder. Det finns
många tillgängliga binäranalysverktyg men några populära binäranalysverktyg är
Ghidra\cite{ghidra_website} och Angr\cite{angr_web}.

% Sure I am referencing to the Ghidra website, but this is done by others as
% well because Ghidra has no report afaik. An example report that uses ghidra
% https://rp.os3.nl/2019-2020/p49/report.pdf

% Vidare har angr t.ex. sagt att "om man använder angr som en komponent i sin
% projekt ska man helst citera en viss rapport", men vi beskriver bara med
% några ord vad angr är och använder den till inget.

Ghidra är ett \emph{reverse engingeering} ramverk utvecklat av USA:s NSA
(National Security Agency) och kan disassembla en binär till pseudo-C-kod.
Ghidra har också en debugger och funktionsgraf. Debuggern ska underlätta binär
debugging genom att integrera med andra funktioner i Ghidra. Funktionsgrafen
låter användaren se hur programmet är uppbyggt visuellt och hur olika
funktioner interagerar med varandra. Funktionaliteter kan utökas eller andra
funktioner utvecklas genom plugins till Ghidra\cite{ghidra_use_cases}. Ghidra
tillåter även automatisering genom att skriva skript. Ett exempel är ett skript
som hittar exempelvis sårbarhet i form av funktionsanropp till potentiellt
osäkra API-anrop genom statisk analys\cite{ghidra_script}.

Angr är en binäranalysverktyg med stöd för både statiska och dynamiska analyser
med hjälp av symbolisk exekvering. Angr har, som Ghidra, stöd för
disassemblering till pseudo-C-kod och många analyser man kan utföra. Angr är
baserat på en emulator skriven i Python med stöd för symbolisk exekvering och
analyser utförs genom Python-skript som interagerar med Angrs API. Angr har
använts för att framställa skript som kan utföra \emph{reverse engineering},
sårbarhetssökning och fungera som exploateringsverktyg\cite{angr_docs}.
